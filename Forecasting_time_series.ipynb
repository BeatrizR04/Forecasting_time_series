{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5b9cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6003342",
   "metadata": {},
   "source": [
    "### Proceso\n",
    "- Disponemos de unos datasets donde vemos las ventas de aceite en Ecuador\n",
    "- El usuario elige un cluster (tiendas similares) para hacer los análisis\n",
    "- Análisis principal: Se van a predecir las ventas de aceite a nivel global. Es decir, ventas totales en un día en todas las tiendas\n",
    "    - Para ello necesitamos agrupar la serie por fecha y añadirle variables que nos vayan a influir en el cálculo\n",
    "    - Se realizan diversos cálculos para ver cómo es la serie y analizarla\n",
    "- Funciones de modelado: Se van a crear funciones que cargen los modelos y predigan el resultado\n",
    "    - Necesitamos funciones para cada modelo\n",
    "    - Se 'repite' algún modelo con parámetros distintos\n",
    "    - Se ejecuta el resultado\n",
    "- Tabla de resultados: Se va a generar una tabla con métricas que nos indicará los resultados de las predicciones hechas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84acd0a1",
   "metadata": {},
   "source": [
    "### Lectura de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8db8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos los archivos\n",
    "df_oil = pd.read_csv('oil.csv')\n",
    "df_holidays = pd.read_csv('holidays_events.csv')\n",
    "df_stores = pd.read_csv('stores.csv')\n",
    "df_transactions = pd.read_csv('transactions.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Consolidamos los datos para tener toda la información en una solo df\n",
    "df = pd.merge(df_train, df_oil, how = 'left', on='date')\n",
    "df = pd.merge(df, df_holidays, how = 'left',on = 'date')\n",
    "df = pd.merge(df, df_transactions, how ='left', on =['date','store_nbr'])\n",
    "df = pd.merge(df, df_stores, how = 'left', on = 'store_nbr')\n",
    "df.rename(columns={'type_x':'holiday_type', 'type_y':'store_type'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1f279",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponemos fecha en formato datetime y la ponemos en el índice\n",
    "df.date = [datetime.strptime(x, '%Y-%m-%d') for x in df.date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367027a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizamos los valores nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c2717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos deshacemos de los valores nulos porque esas variables no nos interesan\n",
    "df['dcoilwtico'] = df['dcoilwtico'].fillna(method='bfill')\n",
    "df[['holiday_type']] = df[['holiday_type']].fillna(0)\n",
    "df['locale'] = df['locale'].fillna(0)\n",
    "df['locale_name'] = df['locale_name'].fillna(0)\n",
    "df['description'] = df['description'].fillna(0)\n",
    "df['transferred'] = df['transferred'].fillna(0)\n",
    "df['transactions'] = df['transactions'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93bb01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que ya no tenemos nulos\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a81ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7689417",
   "metadata": {},
   "source": [
    "### Elección del cluster a analizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1d6eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "select_cluster = input('¿Qué cluster quieres analizar?\\n Las opciones son: 1,2,..,17\\n El cluster a analizar es: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aad2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.cluster == int(select_cluster)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60202e7a",
   "metadata": {},
   "source": [
    "## Análisis principal\n",
    "> **VENTA DE ACEITE POR DÍA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da1a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupamos por fecha y total de ventas\n",
    "df1 = df.groupby('date', as_index = False).sales.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6e95c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la serie a través del tiempo\n",
    "df1.set_index('date').sales.plot(figsize=(15,5), title=f'VENTAS TOTALES DE ACEITE EN EL TIEMPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1243a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos visualizar la serie por años\n",
    "fig, ax = plt.subplots(5,1, figsize=(12,8))\n",
    "ax[0].plot(df1.set_index('date').sales[df1.set_index('date').index < '2014-01-01'], label='2013', color='purple')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[1].plot(df1.set_index('date').sales[(df1.set_index('date').index >= '2014-01-01') &\\\n",
    "                                      (df1.set_index('date').index < '2015-01-01')], label='2014', color='blue')\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[2].plot(df1.set_index('date').sales[(df1.set_index('date').index >= '2015-01-01') &\\\n",
    "                                      (df1.set_index('date').index < '2016-01-01')], label='2015', color='yellow')\n",
    "ax[2].legend(loc='upper left')\n",
    "ax[3].plot(df1.set_index('date').sales[(df1.set_index('date').index >= '2016-01-01') &\\\n",
    "                                      (df1.set_index('date').index < '2017-01-01')], label='2016', color='black')\n",
    "ax[3].legend(loc='upper left')\n",
    "ax[4].plot(df1.set_index('date').sales[df1.set_index('date').index >= '2017-01-01'], label='2017', color='green')\n",
    "ax[4].legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76b002",
   "metadata": {},
   "source": [
    "#### Descomposición de la serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513d935",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tendencia\n",
    "trend = sm.tsa.seasonal_decompose(df1.set_index('date')['sales'], model='additive', period=365).trend\n",
    "\n",
    "# Estacionalidad\n",
    "seasonal = sm.tsa.seasonal_decompose(df1.set_index('date')['sales'], model='additive', period=365).seasonal\n",
    "\n",
    "# Residuos\n",
    "resid = sm.tsa.seasonal_decompose(df1.set_index('date')['sales'], model='additive', period=365).resid\n",
    "\n",
    "# Graficamos\n",
    "fig, ax = plt.subplots(4,1, figsize=(12,10))\n",
    "ax[0].plot(df1.set_index('date')['sales'], label='Ventas en el tiempo')\n",
    "ax[0].legend(loc='upper left')\n",
    "ax[1].plot(trend, label='Tendencia')\n",
    "ax[1].legend(loc='upper left')\n",
    "ax[2].plot(seasonal, label='Estacionalidad')\n",
    "ax[2].legend(loc='upper left')\n",
    "ax[3].plot(resid, label='Residuo')\n",
    "ax[3].legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Se podría aplicar también la función seasonal_decompose y se obtiene todo a la vez. Es lo mismo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2794bde4",
   "metadata": {},
   "source": [
    "#### Autocorrelación y estacionareidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da0a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF y PACF (se pueden utilizar para sleccionar los parámetros de los modelos ARIMA)\n",
    "plot_acf(df1.sales)\n",
    "plot_pacf(df1.sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a158b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test de Dickey Fuller (Para comprobar la estacionariedad)\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "adf, pval, usedlag, nobs, crit_vals, icbest =  adfuller(df1.sales)\n",
    "print('ADF test statistic:', adf)\n",
    "print('ADF p-values:', pval)\n",
    "print('ADF number of lags used:', usedlag)\n",
    "print('ADF number of observations:', nobs)\n",
    "print('ADF critical values:', crit_vals)\n",
    "print('ADF best information criterion:', icbest)\n",
    "\n",
    "# Si el p-valor es menor que 0.05, es estacionaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ed794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tenemos que diferenciarla para que sea estacionaria\n",
    "df1['new_sales'] = df1.sales.diff()\n",
    "df1 = df1.dropna()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a aplicar la prueba de Dickey-Fuller\n",
    "adf, pval, usedlag, nobs, crit_vals, icbest =  adfuller(df1.new_sales)\n",
    "print('ADF test statistic:', adf)\n",
    "print('ADF p-values:', pval)\n",
    "print('ADF number of lags used:', usedlag)\n",
    "print('ADF number of observations:', nobs)\n",
    "print('ADF critical values:', crit_vals)\n",
    "print('ADF best information criterion:', icbest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bd0795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos columnas de mes y día de la semana\n",
    "df1['month'] = df1['date'].apply(lambda x: x.strftime('%B'))\n",
    "df1['week_day'] = df1['date'].dt.day_name()\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7028a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos la distribucción de las ventas por mes y día de la semana\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\n",
    "sns.boxplot(x=df1.month, y=df1.new_sales, data=df1, ax=axes[0])\n",
    "sns.boxplot(x=df1.week_day, y=df1.new_sales, data=df1, ax=axes[1])\n",
    "\n",
    "\n",
    "axes[0].set_title('Distribucción por mes', fontsize=18); \n",
    "axes[1].set_title('Distribucción por día de la semana', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca89a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estos outliers van a 'molestar' a la hora de modelar, por lo que vamos a cambiar sus valores por las medias de esos meses\n",
    "\n",
    "# Media de cada mes\n",
    "monthly_means = df1.groupby('month')['sales'].mean()\n",
    "\n",
    "# Obtener el bigote superior para cada mes\n",
    "boxplot_stats = df1.groupby('month')['sales'].describe()['75%']\n",
    "upper_whisker = boxplot_stats + 1.5 * (boxplot_stats - df1.groupby('month')['sales'].describe()['25%'])\n",
    "\n",
    "# Crear una máscara que seleccione los valores atípicos\n",
    "mask = df1.sales > upper_whisker[df1.month.values].values\n",
    "\n",
    "# Reemplazar los valores atípicos con la media del mes\n",
    "df1.sales[mask] = df1.groupby('month')['sales'].transform('mean')\n",
    "\n",
    "# Hemos buscado el valor del bigote superior para coger todos los outliers \n",
    "# Límite inferior del bigote = Q1 - 1.5 * RI\n",
    "# Límite superior del bigote = Q3 + 1.5 * RI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ccf16b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualizamos la distribucción de las ventas por mes y día de la semana después de quitar los valores atípicos\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20,7), dpi= 80)\n",
    "sns.boxplot(x=df1.month, y=df1.sales, data=df1, ax=axes[0])\n",
    "sns.boxplot(x=df1.week_day, y=df1.sales, data=df1, ax=axes[1])\n",
    "\n",
    "\n",
    "axes[0].set_title('Distribucción por mes', fontsize=18); \n",
    "axes[1].set_title('Distribucción por día de la semana', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569a589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a añadir como variables ventanas temporales (medias móviles) para usarlas a la hora de predecir\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df1['Wind_1day'] = df1.new_sales.rolling(2).mean().reset_index(0,drop=True)\n",
    "df1['Wind_7day'] = df1.new_sales.rolling(8).mean().reset_index(0,drop=True)\n",
    "df1['Wind_14day'] = df1.new_sales.rolling(15).mean().reset_index(0,drop=True)\n",
    "df1['Wind_21day'] = df1.new_sales.rolling(22).mean().reset_index(0,drop=True)\n",
    "df1['Wind_month'] = df1.new_sales.rolling(31).mean().reset_index(0,drop=True)\n",
    "# Las siguientes son medias móviles del mismo día que estás mirando pero de las semanas anteriores\n",
    "# Última semana\n",
    "df1['last_week_avg'] = df1.groupby('week_day')['sales'].rolling(window=7).mean().reset_index(0, drop=True)\n",
    "# Últimas dos semanas\n",
    "df1['two_week_avg'] = df1.groupby('week_day')['sales'].rolling(window=14).mean().reset_index(0, drop=True)\n",
    "# Últimas tres semanas\n",
    "df1['three_week_avg'] = df1.groupby('week_day')['sales'].rolling(window=21).mean().reset_index(0, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las primeras filas para quitarnos los nulos y no tener problemas con los modelos. \n",
    "# Al ser pocos datos no van a afectarnos mucho\n",
    "df1.dropna(inplace=True)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e722be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antes de entrenar los modelos, vamos a hacer dummies en las columnas categóricas ya que la mayoría de los modelos\n",
    "# solo admiten numéricas\n",
    "df1_dummies = pd.concat([df1, pd.get_dummies(df1[['month','week_day']])], axis=1)\n",
    "df1_dummies.drop(columns=['month','week_day'], inplace = True)\n",
    "df1_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b90cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a separar los datos de entrenamiento y test de tal manera que lo que vamos a predecir sea el último año y creamos\n",
    "# una columna para facilitarnos la separación\n",
    "df1_dummies['set'] = ['train' if x <= datetime(2017,1,1) else 'test' for x in df1_dummies.date]\n",
    "\n",
    "df1_dummies.set_index('date', drop=True, inplace=True)\n",
    "df1_dummies.index.name = None\n",
    "\n",
    "X_train1_dummies = df1_dummies[df1_dummies.set == 'train'].drop(columns=['sales','set','new_sales'])\n",
    "X_test1_dummies =df1_dummies[df1_dummies.set == 'test'].drop(columns=['sales','set','new_sales'])\n",
    "y_train1_dummies = df1_dummies[df1_dummies.set == 'train'].new_sales\n",
    "y_test1_dummies = df1_dummies[df1_dummies.set == 'test'].new_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc0e43d",
   "metadata": {},
   "source": [
    "### Material utilizado en los modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92daca76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a generar un df con los resultados de los distintos modelos para compararlos\n",
    "# Lo generamos antes de elegir el cluster por si elegimos distintos en la misma ejecucción que se guarden todos\n",
    "results = pd.DataFrame(columns = ['Model', 'Cluster', 'MAE', 'MAPE', 'MSE', 'RMSE', 'R2 Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5153b07f",
   "metadata": {},
   "source": [
    "## MODELAJE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a4fb3",
   "metadata": {},
   "source": [
    "### Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04d8bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X_train, y_train, X_test, y_test):\n",
    "    # Entrenamos el modelo\n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "            \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_lr, label='Pronóstico Regresión lineal')\n",
    "    plt.title('Pronóstico Regresión lineal')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_lr\n",
    "\n",
    "def linear_regression_normalize(X_train, y_train, X_test, y_test):\n",
    "    # Entrenamos el modelo y normalizamos las variables de entrada\n",
    "    lr = LinearRegression(normalize=True)\n",
    "    lr.fit(X_train, y_train)\n",
    "    y_pred_lr = lr.predict(X_test)\n",
    "            \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_lr, label='Pronóstico Regresión lineal')\n",
    "    plt.title('Pronóstico Regresión lineal con variables de entrada normalizadas')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_lr\n",
    "\n",
    "def random_forest(X_train, y_train, X_test, y_test):\n",
    "    # Entrenamos el modelo\n",
    "    rf = RandomForestRegressor(random_state=0)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "            \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_rf, label='Pronóstico Random Forest')\n",
    "    plt.title('Pronóstico Random Forest')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_rf\n",
    "\n",
    "def random_forest_ntrees(X_train, y_train, X_test, y_test):\n",
    "    # Entrenamos el modelo y seleccionamos el número de árboles en el bosque y la profundidad max de cada árbol\n",
    "    # Cuántos más árboles más se ajustaría el modelo\n",
    "    rf = RandomForestRegressor(random_state=0, n_estimators = 20, max_depth = 5)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred_rf = rf.predict(X_test)\n",
    "        \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_rf, label='Pronóstico Random Forest')\n",
    "    plt.title('Pronóstico Random Forest con elección del número de árboles y su profundidad')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_rf\n",
    "\n",
    "def xgboost(X_train, y_train, X_test, y_test):\n",
    "    # Entrenamos el modelo\n",
    "    xgb = XGBRegressor()\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "        \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_xgb, label='Pronóstico XGB')\n",
    "    plt.title('Pronóstico XGBoost')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_xgb\n",
    "\n",
    "\n",
    "def xgboost_subs(X_train, y_train, X_test, y_test):\n",
    "    # Entrenar el modelo y elegimos las muestras para entrenar cada árbol. Un valor menor a 1.0 puede hacer que el \n",
    "    #modelo sea más robusto al reducir el riesgo de sobreajuste, pero también puede disminuir el rendimiento en algunos \n",
    "    #conjuntos de datos.\n",
    "    xgb = XGBRegressor(subsample = 0.8)\n",
    "    xgb.fit(X_train, y_train)\n",
    "    y_pred_xgb = xgb.predict(X_test)\n",
    "    \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_xgb, label='Pronóstico XGB')\n",
    "    plt.title('Pronóstico XGBoost con muestreo')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return y_pred_xgb\n",
    "\n",
    "\n",
    "def sarima(X_train, y_train, X_test, y_test, order, seasonal_order):\n",
    "    # Entrenamos y ajustamos el modelo\n",
    "    # order son las variables (p,d,q) y seasonal_order las variables (P,D,Q,S) del modelo SARIMAX\n",
    "    sarima = SARIMAX(endog=y_train, order=order, seasonal_order=seasonal_order)\n",
    "    sarima_fit = sarima.fit()\n",
    "    y_pred_sarima = sarima_fit.forecast(steps=len(X_test))\n",
    "    \n",
    "    # Graficar los datos de prueba y el pronóstico\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(X_test.index, y_test, label='Datos de prueba')\n",
    "    plt.plot(X_test.index, y_pred_sarima, label='Pronóstico SARIMA')\n",
    "    plt.title('Pronóstico SARIMA')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Ventas')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return y_pred_sarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b60ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models = {'Regresión lineal': linear_regression, 'Regresión lineal normalizada': linear_regression_normalize, \n",
    "               'Random Forest': random_forest, 'Random Forest con nº de árboles' : random_forest_ntrees,\n",
    "               'XGBoost': xgboost, 'XGBoost con muestreo' : xgboost_subs, 'SARIMA' : sarima}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a04146",
   "metadata": {},
   "source": [
    "### Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f413539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecast(df, model_name, X_train, y_train, X_test, y_test, results, order=None, seasonal_order=None):\n",
    "    model = dict_models[model_name]\n",
    "    if model_name == 'SARIMA':\n",
    "        y_pred = model(X_train, y_train, X_test, y_test, order, seasonal_order)\n",
    "    else:\n",
    "        y_pred = model(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    #Calculamos algunas de sus métricas\n",
    "    mae = metrics.mean_absolute_error(y_test, y_pred)\n",
    "    mape = metrics.mean_absolute_percentage_error(y_test, y_pred)\n",
    "    mse = metrics.mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\n",
    "    r2 = metrics.r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Creamos diccionario con métricas del modelo\n",
    "    model_metrics = {\n",
    "        'Model': model_name,\n",
    "        'Cluster' : select_cluster,\n",
    "        'MAE': mae,\n",
    "        'MAPE': mape,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R2 Score': r2,\n",
    "        'Order' : order,\n",
    "        'Seasonal order' : seasonal_order\n",
    "    }\n",
    "    \n",
    "    # Agregamos una fila a la tabla de resultados\n",
    "    results = results.append(model_metrics, ignore_index = True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c1170",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# En esta linea probamos los distintos modelos y se van guardando\n",
    "# Importante tener en cuanta que si elegimos el XGB con categóricas hay que meter los datos sin dummies\n",
    "results = forecast(df1, 'SARIMA', X_train1_dummies, y_train1_dummies, X_test1_dummies, y_test1_dummies,\\\n",
    "                   results, order = (1,1,1), seasonal_order = (1,1,1,12))\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278569e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480b511b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
